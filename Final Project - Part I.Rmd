---
output:
  html_document: default
  word_document: default
---
#Final Project Report 
Luke Sturgeon  
Stat 517  
11/15/18  

###Introduction
Machine learning is a term thrown about loosely in everyday discussions of data science. It has floated from the realm of statistics into some form of cure all, with people believing it can solve any problem. Machine learning is in fact simply building mathematical models to help understand what is going on in a dataset. The learning aspect becomes salient when that model has parameters that can be tuned to more closely follow the patterns that may emerge in the data (Vanderplas, 2016).  After tuning this data, we can begin predicting which category it may fall into for categorical/ordinal data types, and where the new observation may fall for continuous data types as well. 

Broadly, there are two types of machine learning: supervised and unsupervised. The main difference between the two is whether you know your target variable or not. If you know whether an injury were caused by fatigue, there would be a variable in the dataset specifying that, which can be used to train a supervised learning model and even judge how well that model performs by measuring how many observations that models correctly puts into whichever category. For the purposes of this report and for the sake of brevity, this report will explain only unsupervised methods before moving on to how these methods have been implemented on the AII dataset. 
	
Unsupervised learning is used when the data set does not specify the target variable. Unsupervised techniques attempt to find trends within the data and cluster them into groups (clustering), order them in some fashion (seriation), or find rules that relate one value to another (association). 

###Project Goals
Spokane Mining and Research Division (SMRD) is interested in how fatigue factors into workplace accidents. Since miners are exposed to a unique environment, these conditions may affect how workers respond to fatigue. Specifically, the team is interested in gleaning information from the narrative fields that may tell us if the accident was related to fatigue. Since mine operators, safety personal, and the miners themselves have a strong incentive not to report on the job fatigue, this will have to be done in a more nuanced way.

I plan to approach this problem with unsupervised learning. Unsupervised learning takes data and tries to see how the data groups itself. It does not need to have the data specified (i.e. have a "Fatigue" column with 0, 1 whether it does or doesn't have to do with fatigue), it's simply looking for trends. Vectorizing the narrative fields and removing any unnecessary words (either they are filler words, or they explain too little variance, decided through PCA [Principle Component Analysis]) will create a second data set. This dataset will be clustered through various methods, and the method that provides the clearest groupings will be used going forward with the analysis. Once these groups have been created, term frequency counts and other metrics can be used to see why each observation is like its neighbor. Any groups that appear to have fatigue as a possible culprit will have all the observations in that group separated from the AII reports for further analysis with the other 56 variables in the dataset. These analyses will include supervised and association methods, to see if robust predictive models can be made and what words are likely to be associated with fatigue related injuries, respectively.  

###Data and Packages
```{r, cache=TRUE, message=F, warning=F}
install.packages("mclust",repos="https://ftp.osuosl.org/pub/cran/")
install.packages("dplyr",repos="https://ftp.osuosl.org/pub/cran/")
install.packages("arules",repos="https://ftp.osuosl.org/pub/cran/",dependencies = T)
install.packages("tm",repos="https://ftp.osuosl.org/pub/cran/",dependencies = T)
install.packages("factoextra",repos="https://ftp.osuosl.org/pub/cran/")
install.packages("tidytext",repos="https://ftp.osuosl.org/pub/cran/")
install.packages("stringr", repos="https://ftp.osuosl.org/pub/cran/")
install.packages("ggplot2", repos="https://ftp.osuosl.org/pub/cran/")
install.packages("topicmodels",repos="https://ftp.osuosl.org/pub/cran/")
install.packages("RColorBrewer",repos="https://ftp.osuosl.org/pub/cran/")
install.packages("tidyr",repos="https://ftp.osuosl.org/pub/cran/")
library(ggplot2)
library(tidytext)
library(tidyr)
library(factoextra)
library(dplyr)
library(tm)
library(stringr)
library(arules)
library(topicmodels)
library(RColorBrewer)
library(mclust)
```
```{r, message=F, cache=T, warning=F}
accidents <- read.csv("https://www.dropbox.com/s/ayfbrrlo1zujga9/Accidents.txt?dl=1", header=T, sep="|")
```
```{r, message=F, cache=T}
colnames(accidents) <- tolower(colnames(accidents))
accidents$coal_metal_ind <- as.character(accidents$coal_metal_ind)
coal <- filter(accidents, accidents$coal_metal_ind=="C")
coal <- coal[complete.cases(coal),]
dim(coal)
surface <- filter(coal, coal[,"ug_location"]=="NO VALUE FOUND")
underground <- filter(coal, coal[,"ug_location"]!="NO VALUE FOUND")
surface <- surface[,-c(19:22)]
dim(surface)
```
###A Word on EDA
EDA is left for later in this project. The first part of this project is selecting a subset of the larger dataset based on one column of the overall dataset based on the narratives implying fatigue may be related. The classification, regression and (more) unsupervised clustering are then done on this smaller subset of suspected fatigue related reports. Because this topic detection is glorified data pre-processing, EDA is left for the observations we are interested in, within the scope of this project.

###Vectorizing Narratives with TidyText and tm Packages

We start this process by creating a second dataset with the one narrative column then vectorizing. The vectorizing is done both with the "tidytext" and "tm" packages; the former because it fits within the "tidyverse" making visualization much easier with "ggplot2." Explanation of the process is thus much easier. The heavy computation is done with the "tm" package, putting the vectorized matrix into a Document Term Matrix (DTM). This DTM object is what the "topicmodels" package uses when clustering these reports with a Latent Dirichlet allocation (LDA) model.

```{r, message=F, cache=T, warning=F}
text_df <- data_frame(line=1:nrow(surface), text = surface$narrative)
colnames(text_df) <- c("linenumber","text")
text_df$text <- as.character(text_df$text)
text_df %>% unnest_tokens(word, text)
narratives <- text_df %>% 
        mutate(statecode = surface$fips_state_cd, year = surface$cal_yr)
narratives$year <- as.integer(narratives$year)
tidy_narratives <- narratives %>% 
        unnest_tokens(word, text) %>% 
        filter(!str_detect(word, "[0-9]"),!str_detect(word, "[a-z]_"))
data(stop_words)
tidy_narratives <- tidy_narratives %>% 
        anti_join(stop_words)
```

Because I'm curious, let's see what the most common words used in surface coal AII report narratives are.

```{r, message=F, cache=T, warning=F}
tidy_narratives %>% count(word, sort = T)
tidy_narratives %>% count(word, sort = TRUE) %>% 
        filter(n > 6000) %>% 
        mutate(word = reorder(word, n)) %>%  
        ggplot(aes(word, n,color=word,fill=word)) + 
                geom_col() + 
                xlab(NULL) + 
                ggtitle("Most Frequent Words in Surface Coal Narratives") + 
                labs(y="Words", x="Number of Appearances") +
                theme(legend.position="none")
```

Now we create the DTM object of our narrative fields with the "tm" package.

```{r, message=F, cache=T, warning=F}
narratives.tm <- as.matrix(surface$narrative)
row.names(narratives.tm) <- surface$document_no
docs <- Corpus(VectorSource(surface$narrative))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs,stemDocument)
dtm <- DocumentTermMatrix(docs)
dim(dtm)
dtm
```

It's important to note that the sparse matrix isn't exactly 100% sparse. As we'll see later the frequency counts do pull up words so the matrix isn't completely empty. The DTM is just exceedingly sparse, having 0.00162 of the cells with values in them.

```{r, message=F, cache=T, warning=F}
dtm <- removeSparseTerms(dtm, 0.99)
dim(dtm)
dtm
```

For the following analyses to work, all rows within the DTM must be removed.

```{r, message=F, cache=T, warning=F}
rowTotals <- apply(dtm , 1, sum)
dtm   <- dtm[rowTotals> 0, ]
```
Removing words that are found only in 2% of the observations leaves a DTM with 95% sparsity. The word frequencies are shown below as well.

```{r, message=F, cache=T}
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=TRUE)
print(freq[head(ord)])
freq[tail(ord)]
```

###Topic Detection and Feature Selection
We use Latent Dirichlet Allocation (LDA) for topic detection. LDA is a common method used for topic detection in text mining. In broad strokes LDA is based on the concept that every document is a mixture of topics and every topic is a mixture of words. The former principle treats every concept that is detected with this method as a mixture of the number of topics the user specifies for the LDA model. Much like k-means clustering, another unsupervised clustering method, LDA requires the user to specify the number of groups, i.e. topics, that the data, i.e. narrative fields, should be split into. One narrative will then be considered to have 10% of one topic while 0% of another. These percentages are engendered by the second principle. Each topic is composed of the words that compose it. Therefore we can determine how much of a narrative field is given to one topic by how many words that field uses of that topic.  

It is important to note that LDA only groups narratives and words together. It does not give a reason why. It finds words that regularly coincide with each other more commonly than others. Those words that are closer in the vector space formed by the model are grouped together in one cluster, dubbed a "topic" in this case. To tell why these narratives have been lumped together, we must go beyond simply clustering the narratives and move into feature selection, or what makes each topic unique. One common way to do this is with word frequency counts; which words are most salient within the topic. This may not be as helpful as one may think. Part of what makes LDA so attractive is also what makes it weak; it doesn't discretely label each narrative into Topic A or Topic B, but rather gives a percentage confidence that is where the narrative belongs. A narrative can then straddle multiple topics, which can only happen if words are shared between topics. These words, common to multiple topics, are likely to be more common in multiple narratives as well. So by simply looking at what words are most common, we may see that the words that define Topic A and B are the very same words.

A way around this is to look at the words that are less frequently used. These words are more likely to belong to just a few topics, making the job of distinguishing these topics easier. But, if we simply look at the least used words in a topic, we will come to define each topic by a word that is used just once in all the narratives that make up that topic. A balance is then struck between using words that are used commonly enough in narratives to give a fair representation of the topic, but rarely enough to make them specific to only the narratives that rely on those words to convey the topic. This is done with a method known as Term Frequency - Inverse Density Frequency, blessedly shortened to tf-idf. This method weights less frequently used words more than more frequently by multiply the density frequency (rate of use of the most common words) by the inverse density function (where less frequently used words are given more weight).

There are, of course, other methods that can be used for feature selection in text mining. Word vectors have been shown to lead to better performance under supervised learning conditions. In systematic reviews researchers must go through many thousands of article abstracts to see if they should be included in the final analysis. Word vectors have been shown to reduce the amount of abstracts reviewers must manually go through before allowing the model to classify abstracts for inclusion or exclusion by up to 10% (Hashimoto et al., 2016). This may sound modest, but some systematic reviews may have near 30,000 abstracts to pre-screen. A 10% reduction could save tens of hours of researchers time.

```{r, message=F, cache=T, warning=F}
narratives_lda2 <- LDA(dtm, k=2)
lda_topics2 <- tidy(narratives_lda2, matrix="beta")
lda_topics2
```

We can ignore our own advice and look at the most probably words to come from either of the two topics (we specified that there are two topics with k=2 in the code above). Below are the most common words of each topic graphed. It is important to note that the scale of the graphs differ.

```{r, message=F, cache=T, warning=F}
lda_topic_terms2 <- lda_topics2 %>% 
        group_by(topic) %>% 
        top_n(10, beta) %>% 
        ungroup() %>% arrange(topic, -beta)
lda_topic_terms2 %>% 
        mutate(term = reorder(term, beta)) %>% 
        ggplot(aes(term, beta, fill = factor(topic))) + 
                geom_col(show.legend = FALSE) + 
                facet_wrap(~ topic, scales = "free") + 
                ggtitle("Top 10 Most Probable Terms of Each Topic") + 
                labs(x="Words",y="Probability") +
                coord_flip()
```

A more useful visual aid is shown below. The log ratio of the probability the most common words with the greatest difference in probabilities of showing up in topics 1 and 2.

```{r, message=F, cache=T, warning=F}
beta_spread <- lda_topics2 %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
beta_spread
```

```{r, message=F, cache=T, warning=F}
beta_spread_arrange <- arrange(beta_spread, log_ratio)
beta_data <- rbind(beta_spread_arrange[1:10,],beta_spread_arrange[388:397,])
beta_data
ggplot(data=beta_data,aes(x=reorder(term,log_ratio),y=log_ratio)) + 
        geom_col(show.legend = F) + 
        coord_flip() + 
        ggtitle("Words Most Likely to Come From One Topic or the Other") + 
        labs(y="Probability",x="Words in Topic")
```

These words would be most helpful in defining both topics since they are the most probable words in each topic. They appear often enough to well define the topic but are not as likely found in the other topic. This will help pick which topic we believe most likely represents possible fatigue related injuries. To find which narratives belong to this topic, we move on to modelling each narrative as a mixture of these documents.

```{r, message=F, cache=T, warning=F}
lda_docs2 <- tidy(narratives_lda2, matrix= "gamma")
lda_docs2
```

This "gamma" value is the proportion of words in that narrative that come from topic 1. This affords another method of feature selection. We can now arrange the documents by proportion of topics. We can take one document that is almost entirely from one document and see what words are most common within that document. Since this document is a near perfect exemplar of that topic, we can use the most common words within that document to describe what that topic is.

```{r, message=F, cache=T, warning=F}
arrange(lda_docs2,gamma)
arrange(lda_docs2,desc(topic),gamma)
```

According to our tables above, narrative 8373 is almost entirely within topic 2, while narrative 7541 is within topic 1. Pulling up the word frequencies within these documents...

```{r, message=F, cache=T, warning=F}
tidy(dtm) %>%
        filter(document == 8373) %>%
        arrange(desc(count))
tidy(dtm) %>%
        filter(document == 7541) %>%
        arrange(desc(count))
```

It appears that the first topic is dealing with what locations/where the person was when the had their accident whereas the second appears to be dealing with what part of the body was injured. 

Now that we've gone through what two topics look like, let's now look at a more realistic number of topics that are mixed within each narrative. When comparing their abstract pre-screener against an LDA model, Hashimoto et al. set the number of topics to 300. They were also dealing with abstracts from medical and epidimiology journals. Seeing as our narrative fields are much smaller than those abstracts, and they all already speak towards the same topic (injuries), we'll set our topics to 100.

```{r, message=F, cache=T, warning=F}
narratives_lda100 <- LDA(dtm, k=100)
lda_topics100 <- tidy(narratives_lda100, matrix="beta")
lda_topics100
```
```{r, message=F, cache=T, warning=F}
lda_topic_terms100 <- lda_topics100 %>%
        group_by(topic) %>%
        top_n(10, beta) %>%
        ungroup() %>%
        arrange(topic, -beta)
lda_topic_terms100
str(lda_topic_terms100)
dim(lda_topic_terms100)
unique(lda_topic_terms100$topic)
lda_docs100 <- tidy(narratives_lda100, matrix= "gamma")
lda_docs100
```
```{r, message=F, cache=T, warning=F}
narrative_groups <- lda_docs100 %>%
        group_by(document) %>%
        top_n(1,gamma) %>%
        ungroup() %>%
        arrange(topic, -gamma)
narrative_groups
unique(narrative_groups$topic)
```
```{r, message=F, cache=T, warning=F}
head(lda_topic_terms100,1000)
```

As expected, words such as "tired," "sleep," "fatigue," or "naptime" did not pop out of any of the 100 topics. Safety officers and the miners that are the cause of the report both have an incentive not to report an injury was due to fatigue because issues of liability become murky. Instead, these reports tend to point the finger at operational failures; a machine caused the injury rather than a human error. Because of this, I've decided that words that would require attention and focus would be a work around. Words like "driving," "miss," "check," "investigate," and "cause" were all used to select topics that might potentially lead to fatigue related causes of the injuries these narratives report. 

Term frequencies are an inherently skewed way of looking at what topics are speaking too. The most common words within a topic may also be the common words within another, making it harder to distinguish between topic and why observations were split between the topics. Term Frequency - Inverse Document Frequency attempts to correct this by weighting how common the word is within the entire corpus against how often it appears within the topic. Thus, the most frequent words that are the most specific to that topic are selected. We run through the code for this method now in hopes for a more accurate topic selection process. 



```{r, message=F, cache=T, warning=F}
for_count <- left_join( narrative_groups,tidy_narratives, by = c("topic" = "linenumber"))
count_tfidf <- for_count %>%
        group_by(topic) %>%
        count(word, sort= TRUE) %>%
        ungroup()
total_words <- count_tfidf %>%
        group_by(topic) %>%
        summarize(total = sum(n)) %>%
        ungroup()
count_tfidf <- left_join(count_tfidf,total_words)
count_tfidf
tf_idf <- count_tfidf %>%
        bind_tf_idf(word,topic,n)
tf_idf
```

We'll visualize the first six topic to see what words within those topics have the highest tf-idf to show what we've done.

```{r, message=F, cache=T, warning=F}
tf_idf %>%
        select(-total) %>%
        arrange(desc(tf_idf))
```
```{r, message=F, cache=T, warning=F}
tf_idf %>%
        filter(topic %in% c(1:6)) %>%
        arrange(desc(tf_idf)) %>%
        mutate(word = factor(word, levels = rev(unique(word)))) %>% 
        group_by(topic) %>% 
        top_n(15) %>% 
        ungroup %>%
        ggplot(aes(word, tf_idf, fill = topic)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = "tf-idf") +
        facet_wrap(~topic, ncol = 2, scales = "free") +
        coord_flip()
```

Now we create a tibble with the ten words with the highest tf-idf for each topic. Topics that are suspected to have to do with fatigue will be chosen from this list by looking at these ten words and deciding if they may have something to do with fatigue.

```{r,message=F, cache=T, warning=F}
tf_idf.investigate <- tf_idf %>%
        group_by(topic) %>%
        arrange(desc(tf_idf))%>%
        top_n(10)%>%
        ungroup()
tf_idf.investigate <- tf_idf %>%
        group_by(topic) %>%
        top_n(10,tf_idf) %>%
        ungroup() %>%
        arrange(topic, -tf_idf)
tf_idf.investigate
unique(tf_idf.investigate$topic)
```

After going through this new list of words from each topic, topics 5, 6, 19, 32, 52, 63, 82, 86, 87, 89, and 90 are suspected to have to do with fatigue. These topics included words such as "notice," "decided," "found", "react", "negotiate" and other words that imply a judgement or decision was made. As discussed before, as fatigue increases, the ability of people to make sound and rationale decisions decreasing. Since these words appear in these topics, it may mean that fatigue was a factor in these injuries since a decision was made that lead to an injury. 

Now that we have a subset of the original surface coal mine AII reports that may be fatigue related, the next step is to go into further analysis, incorporating the rest of the dataset. The other variables documented can be used to help predict conditions where accidents may be more likely. Further studies will include classification by clustering and regression, rule analysis to see what values are often found together, and regression to predict continuous values, such as amount of work missed for any new observations that may be added to this dataset. 
###Association

Knowing which words are associated with what may also shed some light into how each group was formed. We first need to pre-process our "surface" dataset so that it can be mined using "arules" package. The removed variables are either redundant or uninformative.

```{r, message=F, cache=T, warning=F}
unwanted <- c("contractor_id", "accident_dt","mine_id","controller_id","equip_mfr_cd","equip_mfr_name","equip_model_no","classification_cd","accident_type_cd","occupation_cd","activity_cd","injury_source_cd","nature_injury_cd","inj_body_part_cd","narrative","coal_metal_ind","closed_doc_no","subunit_cd","degree_injury_cd","mining_equip_cd","immed_notify_cd","return_to_work_dt","invest_begin_dt","document_no","fiscal_yr","fiscal_qtr","operator_id")
surface.apr <- surface %>% 
        select(-one_of(unwanted))
colnames(surface.apr)
tofactor <- c("fips_state_cd","no_injuries","tot_exper","mine_exper","job_exper","schedule_charge","days_restrict")
surface.apr[,tofactor] <- lapply(surface.apr[,tofactor], factor)
```

"surface" has bee pre-processed into "surface.apr" in regards to the Apriori methods we'll be using soon. We coerce the dataset to a transactions class.

```{r, message=F, warning=F, cache=T}
surface.trans <- as(surface.apr, "transactions")
surface.trans
summary(surface.trans)
```

We then visualize the most important transactions within this data, items that have 15% support or greater. We then look at the rules that associate these variables together.

```{r, message=F, cache=T, warning=F}
rules <- apriori(surface.trans, parameter = list(support=0.001, confidence=0.5, maxlen = 2))
rules
summary(rules)
```

We found a set of 11641 rules that meet our requirements of support >= 0.1% and confidence >= 50%.

```{r, message=F, cache= T, warning=F}
rules.cal_qtr1 <- subset(rules, subset= rhs %in% "cal_qtr=1" & lift > 1.2)
rules.cal_qtr2 <- subset(rules, subset= rhs %in% "cal_qtr=2" & lift > 1.2)
rules.cal_qtr3 <- subset(rules, subset= rhs %in% "cal_qtr=3" & lift > 1.2)
rules.cal_qtr4 <- subset(rules, subset= rhs %in% "cal_qtr=4" & lift > 1.2)
inspect(sort(rules.cal_qtr1, by = "confidence")[1:3])
inspect(sort(rules.cal_qtr2, by = "confidence")[1:3])
inspect(sort(rules.cal_qtr3, by = "confidence")[1:3])
inspect(sort(rules.cal_qtr4, by = "confidence")[1:3])
```

These rules are very interesting. They point to what variables are associated with each other for the specific calendar quarters. We see that for the first quarter of the year, ice as the injury source was highly associated. That makes sense seeing as the first quarter starts in January. These rules will lead to further investigation of how the different categories within each variable relate to each other. 

###Regression, Classification, and Clustering

Regression, classification, and clustering were done in python. This is the small amount of code needed to preprocess for python. The .csv files were then uploaded to my personal Dropbox.org account (github refused to upload such large datasets). Because the outputs from the following codes are available within the python code in the following sections, these lines have been commented out so that you don't have to deal with random .csv files popping up in your working directory.

```{r, message=F, cache=T, warning=F}
# surface.fatigue.dayslost <- surface.apr
# surface.fatigue.dayslost$days_lost <- ifelse(surface.fatigue.dayslost$days_lost==0,0,1)
# write.csv(surface.fatigue.dayslost, file="dayslost.csv")
# surface.fatigue.totexper <- surface.apr
# surface.fatigue.dayslost$tot_exper <- ifelse(surface.fatigue.dayslost$tot_exper>=5,0,1)
# write.csv(surface.fatigue.dayslost, file="totexp.csv")
# surface.fatigue.totexper.5cats <- surface.apr
# surface.fatigue.totexper.5cats$tot_exper <- replace(surface.fatigue.totexper.5cats$tot_exper, surface.fatigue.totexper.5cats$tot_exper >= 0 & surface.fatigue.totexper.5cats$tot_exper <= 3, 0)
# surface.fatigue.totexper.5cats$tot_exper <- replace(surface.fatigue.totexper.5cats$tot_exper, surface.fatigue.totexper.5cats$tot_exper > 3 & surface.fatigue.totexper.5cats$tot_exper <= 8, 1)
# surface.fatigue.totexper.5cats$tot_exper <- replace(surface.fatigue.totexper.5cats$tot_exper, surface.fatigue.totexper.5cats$tot_exper > 8 & surface.fatigue.totexper.5cats$tot_exper <= 15, 2)
# surface.fatigue.totexper.5cats$tot_exper <- replace(surface.fatigue.totexper.5cats$tot_exper, surface.fatigue.totexper.5cats$tot_exper > 15 & surface.fatigue.totexper.5cats$tot_exper <= 25, 3)
# surface.fatigue.totexper.5cats$tot_exper <- replace(surface.fatigue.totexper.5cats$tot_exper, surface.fatigue.totexper.5cats$tot_exper > 25 & surface.fatigue.totexper.5cats$tot_exper <= 50, 4)
# write.csv(surface.fatigue.totexper.5cats, file="totexpcats.csv")
```
